# -*- coding: utf-8 -*-
"""CONSOLIDADO7_FINALFINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZdkeY2zsSiK3mkOmf2BPhntMNTP2AuJ3
"""

import pandas as pd
import numpy as np


#1. Carga y exploración de datos (1 punto)
#• Carga el dataset proporcionado, que contiene información sobre la popularidad de distintos géneros musicales en países como
#Chile, EE.UU., México, Corea, Japón, Alemania, Rusia e Italia.
url = "https://raw.githubusercontent.com/Aconstanza/Portafolio/refs/heads/main/dataset_generos_musicales.csv"
df = pd.read_csv(url)

#Inspección inicial
print("Información del dataset:")
print(df.info(), "\n")
print("Valores faltantes por columna: ")
print(df.isnull().sum(), "\n")
df_cleaned = df.dropna()
print(df_cleaned)
print("Duplicados en el dataset: ")
print(df.duplicated().sum(), "\n")
df_cleaned = df_cleaned.drop_duplicates()
print(df_cleaned)

#Analiza las características del dataset, identificando distribuciones y tendencias iniciales.
import matplotlib.pyplot as plt
import seaborn as sns

#Mapa de calor de correlaciones entre géneros
plt.figure(figsize=(12,8))
sns.heatmap(df_cleaned.drop(columns='País').corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title("Mapa de calor de correlaciones entre géneros musicales")
plt.show()

#Boxplot de Distribución de la popularidad de los géneros musicales
plt.figure(figsize=(12,8))
sns.boxplot(data=df_cleaned.drop(columns='País').T)
plt.title("Distribución de la popularidad de los géneros musicales")
plt.xlabel("Géneros musicales")
plt.ylabel("Popularidad")
plt.xticks(ticks=np.arange(len(df_cleaned.drop(columns='País').columns)), labels=df_cleaned.drop(columns='País').columns, rotation=45)
plt.show()

#2. Aplicación de algoritmos de clusterización (5 puntos)

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

#Preprocesamiento
scaler = StandardScaler()
df_scaled_array = scaler.fit_transform(df_cleaned.drop(columns='País'))
print(df_scaled_array[:5])

df_scaled = pd.DataFrame(df_scaled_array, columns=df_cleaned.drop(columns='País').columns)

#K-Means:
#Aplica el algoritmo K-Means con un valor inicial de K=3.
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters_k3 = kmeans.fit_predict(df_scaled)
labels = kmeans.labels_
centroides = kmeans.cluster_centers_

# Scatter plot
plt.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], c=labels, label=labels)
plt.scatter(centroides[:, 0], centroides[:, 1], marker='X', s=200, c='red', label='Centroides')
plt.title("Clusterización con K-Means = 3")
plt.xlabel("Género")
plt.ylabel("Popularidad")

unique_labels = set(labels)
for label in unique_labels:
    plt.scatter([], [], c=[plt.cm.viridis(label / len(unique_labels))], label=f'Cluster {label}')
plt.legend()
plt.show()


df_scaled["Cluster_KMeans"] = clusters_k3
print(df_scaled)
print("Clusters asignados (K=3):")
print(df_scaled[["Cluster_KMeans"]])

inertia = []
silhouette_scores = []
K_range = range(2, len(df_scaled))

for k in K_range:
  kmeans =KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)
  labels=kmeans.fit_predict(df_scaled.drop(columns='Cluster_KMeans'))
  inertia.append(kmeans.inertia_)
  if k < len(df_scaled):
    silhouette_scores.append(silhouette_score(df_scaled.drop(columns='Cluster_KMeans'), labels))


#Determina el valor óptimo de K utilizando el método del codo y el coeficiente de silueta.

#Grafico de metodo del codo
plt.figure(figsize=(12,8))
plt.plot(K_range, inertia, marker='o')
plt.xlabel('Número de clusters (K)')
plt.ylabel('Inercia')
plt.title('Método del codo')
plt.grid(True)
plt.show()

# Grafico de Silhouette Score
plt.figure(figsize=(12,8))
plt.plot(range(2, len(df_scaled)), silhouette_scores, marker='o')
plt.xlabel('Número de clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score para diferentes K')
plt.grid(True)
plt.show()

kmeans_k4 = KMeans(n_clusters=4, random_state=42, n_init=10)
clusters_k4 = kmeans_k4.fit_predict(df_scaled)
labels_k4 = kmeans_k4.labels_
centroides_k4 = kmeans_k4.cluster_centers_

# Scatter plot con N° de Clusters modificado
plt.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], c=labels_k4, label=labels_k4)
plt.scatter(centroides_k4[:, 0], centroides_k4[:, 1], marker='X', s=200, c='red', label='Centroides')
plt.title("Clusterización con K-Means = 4")
plt.xlabel("Género")
plt.ylabel("Popularidad")

unique_labelsk4 = set(labels_k4)
for label in unique_labelsk4:
    plt.scatter([], [], c=[plt.cm.viridis(label / len(unique_labelsk4))], label=f'Cluster {label}')
plt.legend()
plt.show()

df_scaled["Cluster_KMeans_k4"] = clusters_k4
print(df_scaled)
print("Clusters asignados (K=4):")
print(df_scaled[["Cluster_KMeans_k4"]])

#Clustering jerárquico:
#Aplica clustering jerárquico y compara con los resultados de K-Means.
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering

hc = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')
labels_hc = hc.fit_predict(df_scaled.drop(columns=['Cluster_KMeans','Cluster_KMeans_k4']))

df_scaled["Cluster_HC"] = labels_hc

plt.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], c=labels_hc, label=labels_hc, cmap='plasma', marker='o')
plt.title("Clusterización con Clustering Jerárquico")
plt.xlabel("Género")
plt.ylabel("Popularidad")
plt.show()

print(df_scaled)
print("Clusters asignados (HC):")
print(df_scaled[["Cluster_HC"]])

#Genera un dendograma y determina el número óptimo de clusters.
plt.figure(figsize=(12,8))
linkage_matrix = sch.linkage(df_scaled.drop(columns=['Cluster_KMeans','Cluster_KMeans_k4','Cluster_HC']), method='ward')
dendrogram = sch.dendrogram(linkage_matrix, labels=df_cleaned['País'].values)
plt.title('Dendrograma')
plt.xlabel('Generos')
plt.ylabel('distancia euclidiana')
plt.show()


#DBSCAN:
#Aplica DBSCAN con diferentes valores de eps y MinPts.
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors

#DBSCAN
df_scaled_for_tsne = df_scaled.drop(columns=['Cluster_KMeans', 'Cluster_KMeans_k4', 'Cluster_HC'])
min_samples = 3  # Elegido considerando pocas dimensiones y tamaño del dataset
nbrs = NearestNeighbors(n_neighbors=min_samples)
nbrs.fit(df_scaled_for_tsne)  # Datos sin clusters previos
distances, indices = nbrs.kneighbors(df_scaled_for_tsne)

# Tomamos la distancia al k-ésimo vecino
distances = np.sort(distances[:, min_samples-1])
plt.figure(figsize=(10,6))
plt.plot(distances)
plt.xlabel("Puntos ordenados")
plt.ylabel(f"Distancia al {min_samples}-ésimo vecino más cercano")
plt.title("Gráfico k-dist para determinar eps")
plt.grid(True)
plt.show()



eps_values = [2.0, 2.1, 2.2, 2.4, 2.5]
min_samples_values = [2, 3, 4]

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels_dbscan = dbscan.fit_predict(df_scaled.drop(columns=['Cluster_KMeans','Cluster_KMeans_k4','Cluster_HC']))
        plt.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], c=labels_dbscan, label=labels_dbscan, cmap='plasma', marker='o')
        plt.title(f"DBSCAN con eps={eps} y min_samples={min_samples}")
        plt.xlabel("Género")
        plt.ylabel("Popularidad")
        for i, country in enumerate(df_cleaned['País']):
          plt.text(df_scaled_for_tsne.iloc[i, 0],
             df_scaled_for_tsne.iloc[i, 1],
             country,
             fontsize=9,
             ha='center', va='center')
        plt.show()


#Justifica la elección de los parámetros y analiza si DBSCAN identifica agrupaciones significativas
#Se escogieron eps entre 2.0 y 2.5 observando el gráfico k-dist
#la curva aumentó alrededor de 2.5 indicando el límite entre puntos densos y dispersos
#Y se probó min_samples = 2,3,4 considerando las dimensiones del dataset


##DBSCAN FINAL
eps_final = 2.4
min_samples_final = 2
dbscan_final = DBSCAN(eps=eps_final, min_samples=min_samples_final)
labels_dbscan_final = dbscan_final.fit_predict(df_scaled_for_tsne)
df_scaled['Cluster_DBSCAN'] = labels_dbscan_final

plt.figure(figsize=(12,8))
plt.scatter(df_scaled_for_tsne.iloc[:, 0],
            df_scaled_for_tsne.iloc[:, 1],
            c=labels_dbscan_final,
            cmap='plasma', marker='o')
plt.title(f"DBSCAN Final con eps={eps_final} y min_samples={min_samples_final}")
plt.xlabel("Género")
plt.ylabel("Popularidad")

# Etiquetar países en el gráfico DBSCAN
for i, country in enumerate(df_cleaned['País']):
    plt.text(df_scaled_for_tsne.iloc[i, 0],
             df_scaled_for_tsne.iloc[i, 1],
             country,
             fontsize=9,
             ha='center', va='center')

plt.show()

print("Clusters DBSCAN Final:")
print(df_scaled[['Cluster_DBSCAN']])
#3. Aplicación de reducción de dimensionalidad (3 puntos)
#PCA:
#• Aplica PCA y determina cuántos componentes principales explican al menos el 90% de la varianza.
from sklearn.decomposition import PCA

pca = PCA()
x_pca = pca.fit_transform(df_scaled.drop(columns=['Cluster_KMeans','Cluster_KMeans_k4','Cluster_HC']))
explained_variance = pca.explained_variance_ratio_
print(explained_variance)

plt.figure(figsize=(12,8))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')
plt.xlabel('Número de componentes principales')
plt.ylabel('Varianza explicada')
plt.title('Varianza explicada por cada componente principal')
plt.grid(True)
plt.show()

cumulative_variance = np.cumsum(explained_variance)
n_components = np.argmax(cumulative_variance >= 0.9) + 1
print(f"Número de componentes principales para 90% de la varianza: {n_components}")

#• Visualiza los países en un gráfico bidimensional con las primeras dos componentes principales.
plt.figure(figsize=(12,8))
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=df_scaled['Cluster_KMeans'], cmap='viridis')
plt.title("PCA con 2 componentes principales")
plt.xlabel("Componente principal 1")
plt.ylabel("Componente principal 2")
# Etiquetar países en el gráfico PCA
for i, country in enumerate(df_cleaned['País']):
    plt.text(x_pca[i, 0],
             x_pca[i, 1],
             country,
             fontsize=9,
             ha='center', va='center')
plt.show()



#t-SNE:
#Aplica t-SNE para visualizar la relación entre los países en un espacio de 2D.
from sklearn.manifold import TSNE
#Experimenta con diferentes valores de perplexity y analiza cómo afectan la representación.
df_scaled_for_tsne = df_scaled.drop(columns=['Cluster_KMeans', 'Cluster_KMeans_k4', 'Cluster_HC'])

perplexity_values = [3, 4, 5, 6, 7]
for perplexity in perplexity_values:
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)
    x_tsne = tsne.fit_transform(df_scaled_for_tsne)
    df_tsne = pd.DataFrame(x_tsne, columns=['componente_1', 'componente_2'])
    df_tsne['País'] = df_cleaned['País']

    plt.figure(figsize=(12,8))
    sns.scatterplot(data=df_tsne, x='componente_1', y='componente_2', hue='País')
    plt.title(f"t-SNE con perplexity={perplexity}")
    plt.xlabel('Componente 1')
    plt.ylabel('Componente 2')
    plt.legend()
    plt.grid()
    for i, country in enumerate(df_tsne['País']):
      plt.text(df_tsne.iloc[i, 0],
             df_tsne.iloc[i, 1],
             country,
             fontsize=9,
             ha='center', va='center')
    plt.show()






#4. Análisis de resultados y conclusiones (1 punto)
#Comparación de métodos:
#• Explica las diferencias entre K-Means, clustering jerárquico y DBSCAN. ¿Cuál funcionó mejor
#en este caso y por qué?
##K-means es rapido y eficiente en grandes volumenes de datos, es facil de entender e implementar, y funciona bien cuando los grupos tienen formas circulaeres o esfericas.
## Pero, se deben definir los K de antemano, no funciona bien si los clusters tienen formas irregulares, y es sensible a outliers

###Clustering Jerarquico: no necesita k de antemano, permite visualizar la relacion entre lso datos con un dendrograma
###Pero es mas costoso y dificil de aplicar en grandes volumenes de datos

####DBSCAN:No requiere especificar numero de clusters, funciona bien con formas de clusters irregulares
####Pero no funciona bien con cluster de densidad variable y es mas lento en grandes volumenes de datos.

##En este caso, clustering jerarquico y K-Means entregaron información similar agrupando en 3 cluster,
#con el grafico del codo, silhouette score y dendrograma también se aprecia información similar
#Y DBSCAN fue util para detectar outliers, pero no logró agrupar los países del mismo modo que clustering jerarquico y K-Means

#• Compara los resultados obtenidos con PCA y t-SNE. ¿Cuál técnica permitió visualizar mejor la relación entre los países?

#PCA Permitió mejor visualización al agrupar los países en cluster con colores para cada uno, también sirvió para comprobar que si bien Rusia, Chile y Corea están en un mismo cluster, Corea es más lejano a lso otros dos países.
#T-SNE también entrega visualizaciones interesantes hasta perplexity 5 logrando agrupar los países igual que los metodos anteriores

#Interpretación:
#• ¿Los clusters obtenidos reflejan similitudes culturales o geográficas en la música?

##No, uno tendería a pensar que países como Chile, Mexico y EE.UU, o como Italia, Alemania, Rusia, o Corea y Japón tendrían mas en común, pero todos quedaron en Grupos separados.


#• Relaciona los resultados con tendencias globales en consumo musical.
#A pesar de las diferencia culturales entre los países, se nota una inclinación por géneros mas "juveniles" como lo es la musica urbana (reggaeton y hip-hop), la musica electronica y popera, y el rock.
#Tambien se puede observar como la globalización ha influido en las preferencias musicales.
#Y es importante recordar que EE.UU es el mayor mercado musical del mundo, y las principales plataformas de streaming son de origen estadounidense
#Y la mayoría de los principales generos escuchados a nivel mundial han sido establecidos o promovidos desde EE.UU, indicando una fuerte influencia occidental al homogeneizar las preferencias musicales en otros países

