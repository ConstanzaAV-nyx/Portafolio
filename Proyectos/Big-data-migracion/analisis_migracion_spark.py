# -*- coding: utf-8 -*-
"""999.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14y5YTQLH3Phc0MjkPMXZ43KvKfdeW7He
"""

!pip install pyspark

#INSTRUCCIONES
#1. Carga y exploración de datos (2 puntos)

from pyspark.sql import SparkSession
import pandas as pd

# Import the 'when' function
from pyspark.sql.functions import col, avg, desc, when, sum

# Crear la sesión de Spark
spark = SparkSession.builder.master("local").appName("Migración y Predicción") \
    .getOrCreate()
print("SparkSession creada correctamente")

# Cargar el archivo CSV con pandas desde la URL
url = "https://raw.githubusercontent.com/Aconstanza/Portafolio/refs/heads/main/migraciones.csv"
pd_df = pd.read_csv(url)
#• Carga el dataset proporcionado en Spark.
#• Convierte los datos en un RDD y un DataFrame.
df_spark = spark.createDataFrame(pd_df)
df_spark.show()
rdd = df_spark.rdd
#• Explora los datos: muestra las primeras filas, el esquema y genera estadísticas descriptivas.
# Mostrar las primeras filas del DataFrame
df_spark.show(5)

# Mostrar el esquema del DataFrame
df_spark.printSchema()

# Generar estadísticas descriptivas
df_spark.describe().show()

#2. Procesamiento de datos con RDDs y DataFrames (3 puntos)
#• Aplica transformaciones sobre los RDDs (filter, map, flatMap).
rdd_filtered = rdd.filter(lambda row: row['Población_Origen']> 10000)

rdd_mapped = rdd.map(lambda row: row['Población_Origen']**2 )

rdd_flatmapped = rdd.flatMap(lambda row: (row['Población_Origen'], row['Población_Destino']))
#• Aplica acciones sobre los RDDs (collect, take, count).
rdd_collected = rdd.collect()
rdd_taken = rdd.take(5)
rdd_counted = rdd.count()
#• Realiza operaciones con DataFrames: filtrado, agregaciones y ordenamiento.
from pyspark.sql.functions import col, avg, desc

df_filtered = df_spark.filter(col('Población_Origen') > 10000)

df_aggregated = df_spark.groupBy('Población_Origen').agg(avg('Población_Destino'))
df_sorted = df_spark.orderBy(col('Población_Origen'), ascending=False)

#• Escribe los resultados en formato Parquet.
df_filtered.write.mode("overwrite").parquet("df_filtered.parquet")
df_aggregated.write.mode("overwrite").parquet("df_aggregated.parquet")
df_sorted.write.mode("overwrite").parquet("df_sorted.parquet")
#3. Consultas con Spark SQL (2 puntos)
#• Registra el DataFrame como una tabla temporal.

df_spark.createOrReplaceTempView("migraciones")
#• Realiza consultas sobre los principales países de origen y destino.
resultado = spark.sql("""
  SELECT Origen, Destino, SUM(`Población_Origen`) AS Total_Origen, SUM(`Población_Destino`) AS Total_Destino
  FROM migraciones
  GROUP BY Origen, Destino
  ORDER BY Total_Origen DESC
  LIMIT 10
  """)
resultado.show()

#• Analiza las principales razones de migración por región.
regiones = {
    'México': 'América Latina',
    'Siria': 'Medio Oriente',
    'Venezuela': 'América Latina',
    'India': 'Asia',
    'Argentina': 'América Latina',
    'Alemania': 'Europa',
    'Colombia': 'América Latina',
    'Emiratos Árabes': 'Medio Oriente',
    'España': 'Europa',
    'EEUU': 'América del Norte'
}
df_spark = df_spark.withColumn("Región", col("Origen").cast("string"))
for pais, region in regiones.items():
    df_spark = df_spark.withColumn("Región", when(col("Origen") == pais, region).otherwise(col("Región")))
df_spark.show()

# Perform the aggregation and ordering using DataFrame operations
resultado_df = df_spark.groupBy("Región").agg(
    sum(col("Población_Origen")).alias("Total_Origen"),
    sum(col("Población_Destino")).alias("Total_Destino")
).orderBy(col("Total_Origen").desc())

resultado_df.show()

##Razones de migración por región
razones_region = df_spark.groupBy("Región", "Razón").agg(
    sum(col("Población_Origen")).alias("Total_Origen"),
    sum(col("Población_Destino")).alias("Total_Destino")
).orderBy(col("Total_Origen").desc())

razones_region.show()

#4. Aplicación de MLlib para predicción de flujos migratorios (3 puntos)
#• Convierte los datos en un formato adecuado para MLlib.
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
#• Aplica un modelo de regresión logística para predecir la probabilidad de migración basada
#en factores socioeconómicos.
df_spark=df_spark.withColumn("Migracion", (col("Población_Origen") > 10000).cast("integer"))
assembler = VectorAssembler(
    inputCols=[
        "Población_Origen", "Población_Destino",
        "PIB_Origen", "PIB_Destino",
        "Tasa_Desempleo_Origen", "Tasa_Desempleo_Destino",
        "Nivel_Educativo_Origen", "Nivel_Educativo_Destino"
    ],
    outputCol="features"
)
from pyspark.ml.feature import StandardScaler


df_spark1 = assembler.transform(df_spark)

scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")
scaler_model = scaler.fit(df_spark1)
df_spark_scaled = scaler_model.transform(df_spark1)

#• Evalúa el modelo y analiza su precisión.
train, test = df_spark_scaled.randomSplit([0.7, 0.3], seed=42)
lr = LogisticRegression(featuresCol="features", labelCol="Migracion")
lr_model = lr.fit(train)
predictions = lr_model.transform(test)
predictions.select("Migracion", "prediction", "probability").show(5)

evaluator = BinaryClassificationEvaluator(labelCol="Migracion", metricName="areaUnderROC")
auc = evaluator.evaluate(predictions)
print("AUC:", auc)

pred_correctas = predictions.filter(col("Migracion") == col("prediction")).count()
pred_incorrectas = predictions.filter(col("Migracion") != col("prediction")).count()
total_predicciones = predictions.count()
accuracy = pred_correctas / total_predicciones
print(f"Predicciones correctas: {accuracy:.2f}")